{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a957ef7-df70-40ae-ad35-46a7aa249c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"sql_query\", \"\")\n",
    "dbutils.widgets.text(\"database\", \"\")\n",
    "sql_query = dbutils.widgets.get(\"sql_query\")\n",
    "database = dbutils.widgets.get(\"database\")\n",
    "\n",
    "if not database:\n",
    "  database = \"ifrs17_cur\"\n",
    "\n",
    "spark.sql(f\"USE {database}\")\n",
    "\n",
    "if not sql_query:\n",
    "  sql_query = \"SELECT * FROM data_ifrs17 LIMIT 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ef2628-62a0-4030-8f2e-a282a866d40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87f7932-3ea9-4204-9fe2-52a6bcb30f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "timestamp = str(int(time.time() * 10000))\n",
    "short_hash = hashlib.md5(timestamp.encode()).hexdigest()[:6]\n",
    "base_path = f\"dbfs:/tmp/api_responses/result_{short_hash}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7201fc3-74ad-4595-b977-e3b111c37773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the chunk size (number of records per file)\n",
    "chunk_size = 1000\n",
    "\n",
    "# Split the DataFrame into chunks of 1000 records\n",
    "total_rows = df.count()\n",
    "num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size > 0 else 0)\n",
    "\n",
    "file_paths = []\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    chunk_df = df.limit(end).subtract(df.limit(start))  # Get the chunk\n",
    "\n",
    "    # Write the chunk to a CSV file\n",
    "    chunk_path = f\"{base_path}_part_{i+1}.csv\"\n",
    "    chunk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(chunk_path)\n",
    "\n",
    "    # Retrieve the actual CSV file from the directory (DBFS stores as directory + part file)\n",
    "    files = dbutils.fs.ls(chunk_path)\n",
    "    csv_file = [file.path for file in files if file.path.endswith(\".csv\")][0]\n",
    "    \n",
    "    file_paths.append((csv_file, chunk_df.count()))  # Store the file path and record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary CSV file with file paths and record counts\n",
    "summary_path = f\"{base_path}_summary.csv\"\n",
    "summary_data = [(path, count) for path, count in file_paths]\n",
    "summary_df = spark.createDataFrame(summary_data, [\"file_path\", \"record_count\"])\n",
    "\n",
    "# Write the summary CSV file\n",
    "summary_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(summary_path)\n",
    "\n",
    "# Retrieve the summary file path\n",
    "summary_file = [file.path for file in dbutils.fs.ls(summary_path) if file.path.endswith(\".csv\")][0]\n",
    "\n",
    "# Exit the notebook and return the summary file path\n",
    "dbutils.notebook.exit(summary_file)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2697648055580681,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "return_query_results_api",
   "widgets": {
    "database": {
     "currentValue": "",
     "nuid": "b4d7ae36-22bf-4570-93e6-cd2d12dadc96",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "sql_query": {
     "currentValue": "",
     "nuid": "894807d3-e76e-4c16-b287-1c05d6d7e4f8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sql_query",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "sql_query",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
