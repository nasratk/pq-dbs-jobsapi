{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a957ef7-df70-40ae-ad35-46a7aa249c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"sql_query\", \"\")\n",
    "dbutils.widgets.text(\"database\", \"\")\n",
    "sql_query = dbutils.widgets.get(\"sql_query\")\n",
    "database = dbutils.widgets.get(\"database\")\n",
    "\n",
    "if not database:\n",
    "  database = \"ifrs17_cur\"\n",
    "\n",
    "spark.sql(f\"USE {database}\")\n",
    "\n",
    "if not sql_query:\n",
    "  sql_query = \"SELECT * FROM data_ifrs17 LIMIT 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ef2628-62a0-4030-8f2e-a282a866d40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87f7932-3ea9-4204-9fe2-52a6bcb30f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "# Generate a unique hash based on the timestamp\n",
    "timestamp = str(int(time.time() * 10000))\n",
    "short_hash = hashlib.md5(timestamp.encode()).hexdigest()[:6]\n",
    "\n",
    "# Define paths based on naming convention\n",
    "base_folder = f\"dbfs:/tmp/api_responses/result_{short_hash}.csv/\"\n",
    "summary_file = f\"dbfs:/tmp/api_responses/result_{short_hash}_summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7201fc3-74ad-4595-b977-e3b111c37773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition the DataFrame into chunks of approximately 1000 records\n",
    "chunk_size = 1000\n",
    "num_chunks = (df.count() // chunk_size) + 1\n",
    "df_repartitioned = df.repartition(num_chunks)\n",
    "\n",
    "# Write the data in chunks to DBFS\n",
    "df_repartitioned.write.mode(\"overwrite\").option(\"header\", \"true\").csv(base_folder)\n",
    "\n",
    "# List the CSV part files generated for each chunk\n",
    "chunk_files = []\n",
    "folders = dbutils.fs.ls(base_folder)\n",
    "for folder in folders:\n",
    "    if folder.isDir():\n",
    "        part_files = dbutils.fs.ls(folder.path)\n",
    "        chunk_files.extend([file.path for file in part_files if file.path.endswith(\".csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summary CSV with the file paths of all chunk files\n",
    "summary_data = [(file,) for file in chunk_files]\n",
    "\n",
    "# Create a DataFrame for the summary\n",
    "summary_df = spark.createDataFrame(summary_data, [\"csv_file_path\"])\n",
    "\n",
    "# Write the summary file\n",
    "summary_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(summary_file)\n",
    "\n",
    "# Return the path to the summary file\n",
    "dbutils.notebook.exit(summary_file)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2697648055580681,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "return_query_results_api",
   "widgets": {
    "database": {
     "currentValue": "",
     "nuid": "b4d7ae36-22bf-4570-93e6-cd2d12dadc96",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "sql_query": {
     "currentValue": "",
     "nuid": "894807d3-e76e-4c16-b287-1c05d6d7e4f8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "sql_query",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "sql_query",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
